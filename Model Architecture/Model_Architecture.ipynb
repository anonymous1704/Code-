{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT7--_BRvdp8"
      },
      "outputs": [],
      "source": [
        "import time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "CKPT_DIR = Path(\"./checkpoints\"); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_PATH      = \"Final_Proceesed_Dataset.csv\"\n",
        "TRIAL_COL     = \"TrialID\"\n",
        "PARTIC_COL    = \"ParticipantID\"\n",
        "TARGET_X      = \"newCopX\"\n",
        "TARGET_Y      = \"newCopY\"\n",
        "TIME_COLS     = [\"aligned_time\", \"Timestamp\"]\n",
        "\n",
        "DES_LONG_SEC  = 8.0\n",
        "DES_SHORT_SEC = 2.0\n",
        "DES_HOP_SEC   = 0.20\n",
        "DES_HOR_SEC   = 0.06\n",
        "\n",
        "\n",
        "CLEAN_LEVEL   = 1\n",
        "MAD_K         = 3.5\n",
        "HAMPEL_K      = 5\n",
        "HAMPEL_T0     = 3.5\n",
        "\n",
        "INCLUDE_PAST_COP = False\n",
        "USE_DERIVED_FEATS = False\n",
        "\n",
        "USE_AUG          = True\n",
        "AUG_NOISE_STD    = 0.01\n",
        "AUG_TIMEMASK_P   = 0.25\n",
        "AUG_TIMEMASK_LEN = 6\n",
        "\n",
        "RANDOM_SEED   = 42\n",
        "\n",
        "\n",
        "D_MODEL       = 160\n",
        "N_HEADS       = 8\n",
        "N_LAYERS      = 3\n",
        "DROPOUT       = 0.20\n",
        "PATCH_OVERLAP = 0.75\n",
        "SCALES        = (1,2,4)\n",
        "\n",
        "# Train\n",
        "BATCH         = 64\n",
        "EPOCHS        = 100\n",
        "LR            = 3e-4\n",
        "WEIGHT_DECAY  = 5e-4\n",
        "PATIENCE      = 12\n",
        "CLIP_NORM     = 1.0\n",
        "\n",
        "# SWA\n",
        "USE_SWA        = True\n",
        "SWA_START_FRAC = 0.7\n",
        "SWA_LR_FACTOR  = 0.5\n",
        "\n",
        "# Weighted loss\n",
        "WEIGHT_Q          = 0.70\n",
        "SPIKE_WEIGHT      = 2.0\n",
        "AXIS_WEIGHT_FROM_TRAIN_IQR = True\n",
        "\n",
        "#TTA\n",
        "USE_TTA      = False\n",
        "TTA_RUNS     = 5\n",
        "TTA_NOISE    = 0.01\n",
        "\n",
        "# Latency\n",
        "PRINT_LATENCY          = True\n",
        "LAT_WARMUP_STEPS       = 10\n",
        "LAT_MEASURE_STEPS      = 50\n",
        "LAT_WARMUP_SAMPLES_PS  = 20\n",
        "LAT_MEASURE_SAMPLES_PS = 50\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def set_seed(s=RANDOM_SEED):\n",
        "    import random\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def find_time_col(df):\n",
        "    for c in TIME_COLS:\n",
        "        if c in df.columns:\n",
        "            try: df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "            except: pass\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def seconds_to_samples(sec, fs): return max(1, int(round(sec*fs)))\n",
        "\n",
        "def estimate_fs(df, tcol, default=50.0):\n",
        "    if tcol is None or not np.issubdtype(df[tcol].dtype, np.datetime64): return default\n",
        "    dts=[]\n",
        "    for _,g in df.groupby(TRIAL_COL, sort=False):\n",
        "        tt=g[tcol].astype('int64').to_numpy()/1e9; dt=np.diff(tt)\n",
        "        if len(dt): dts.append(np.median(dt))\n",
        "    if len(dts) and np.median(dts)>0: return 1.0/np.median(dts)\n",
        "    return default\n",
        "\n",
        "\n",
        "\n",
        "def find_inputs(df):\n",
        "    prefixes=('hmd_','controller_lefthand_','controller_righthand_','lefthand_','righthand_','hands_')\n",
        "    cols=[c for c in df.columns if any(c.startswith(p) for p in prefixes)]\n",
        "    for c in ['hmd_ang_velocity_Y','controller_lefthand_ang_velocity_Y','controller_righthand_ang_velocity_Y']:\n",
        "        if c in df.columns and c not in cols: cols.append(c)\n",
        "    return sorted(list(dict.fromkeys(cols)))\n",
        "\n",
        "def make_diffs_per_trial_df(df, cols):\n",
        "\n",
        "    grp = df.groupby(TRIAL_COL, sort=False)\n",
        "    out = {}\n",
        "    for c in cols:\n",
        "        out[c+\"_d1\"] = grp[c].diff().fillna(0.0).astype(np.float32)\n",
        "        out[c+\"_d2\"] = grp[c].diff().diff().fillna(0.0).astype(np.float32)\n",
        "    return pd.DataFrame(out, index=df.index)\n",
        "\n",
        "def robust_clip_inplace(df, cols, by=None, k=3.5):\n",
        "    if CLEAN_LEVEL==0 or not cols: return df\n",
        "    if by and by in df.columns:\n",
        "        for _, gidx in df.groupby(by, sort=False).groups.items():\n",
        "            sub = df.loc[gidx, cols]\n",
        "            med = sub.median()\n",
        "            mad = (sub - med).abs().median() * 1.4826 + 1e-9\n",
        "            df.loc[gidx, cols] = sub.clip(med - k*mad, med + k*mad, axis=1)\n",
        "    else:\n",
        "        sub = df.loc[:, cols]\n",
        "        med = sub.median()\n",
        "        mad = (sub - med).abs().median() * 1.4826 + 1e-9\n",
        "        df.loc[:, cols] = sub.clip(med - k*mad, med + k*mad, axis=1)\n",
        "    return df\n",
        "\n",
        "def _hampel_inplace_np(y: np.ndarray, k=5, t0=3.5):\n",
        "    n = y.shape[0]\n",
        "    if k <= 0 or n == 0: return\n",
        "    kk = max(3, int(k))\n",
        "    for i in range(kk, n-kk):\n",
        "        w = y[i-kk:i+kk+1]\n",
        "        med = np.median(w)\n",
        "        mad = 1.4826*np.median(np.abs(w - med)) + 1e-9\n",
        "        if abs(y[i] - med) > t0 * mad:\n",
        "            y[i] = med\n",
        "\n",
        "def hampel_targets_per_trial_inplace(df, tgt_cols, k=5, t0=3.5):\n",
        "    if CLEAN_LEVEL==0 or k<=0: return df\n",
        "    groups = df.groupby(TRIAL_COL, sort=False).groups\n",
        "    for _, idx in groups.items():\n",
        "        for c in tgt_cols:\n",
        "            y = df.loc[idx, c].to_numpy(copy=True, dtype=np.float32)\n",
        "            _hampel_inplace_np(y, k=k, t0=t0)\n",
        "            df.loc[idx, c] = y\n",
        "    return df\n",
        "\n",
        "\n",
        "def positional_encoding(n, d, device):\n",
        "    pe=torch.zeros(n,d,device=device)\n",
        "    pos=torch.arange(0,n,dtype=torch.float,device=device).unsqueeze(1)\n",
        "    div=torch.exp(torch.arange(0,d,2,device=device).float()*(-np.log(10000.0)/d))\n",
        "    pe[:,0::2]=torch.sin(pos*div); pe[:,1::2]=torch.cos(pos*div)\n",
        "    return pe.unsqueeze(0)\n",
        "\n",
        "class SqueezeExcite1D(nn.Module):\n",
        "    def __init__(self, channels: int, r: int = 8):\n",
        "        super().__init__()\n",
        "        m = max(1, channels // r)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc   = nn.Sequential(\n",
        "            nn.Linear(channels, m, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(m, channels, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = x.transpose(1, 2)\n",
        "        w = self.pool(z).squeeze(-1)\n",
        "        w = self.fc(w).unsqueeze(-1)\n",
        "        z = z * w\n",
        "        return z.transpose(1, 2)\n",
        "\n",
        "class ConvStem(nn.Module):\n",
        "    def __init__(self, in_ch, k=7, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dw=nn.Conv1d(in_ch,in_ch,kernel_size=k,padding=k//2,groups=in_ch)\n",
        "        self.pw=nn.Conv1d(in_ch,in_ch,kernel_size=1)\n",
        "        self.act=nn.GELU(); self.bn=nn.BatchNorm1d(in_ch); self.do=nn.Dropout(dropout)\n",
        "        self.se=SqueezeExcite1D(in_ch, r=8)\n",
        "    def forward(self, x):\n",
        "        z=x.transpose(1,2); z=self.dw(z); z=self.pw(z)\n",
        "        z=self.act(z); z=self.bn(z); z=self.do(z)\n",
        "        z=z.transpose(1,2); z=self.se(z)\n",
        "        return z\n",
        "\n",
        "class TimePooledPatch(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, scales=(1,2,4), overlap=0.75):\n",
        "        super().__init__()\n",
        "        self.seq_len=int(seq_len); self.scales=tuple(scales); self.overlap=float(overlap); self.d_model=int(d_model)\n",
        "        self.proj=nn.ModuleList([nn.Identity() for _ in self.scales]); self._inited=[False]*len(self.scales)\n",
        "    def _ensure(self, i, in_feats, dev, dtype):\n",
        "        if (not self._inited[i]) or (not isinstance(self.proj[i], nn.Linear)) or (self.proj[i].in_features!=in_feats):\n",
        "            self.proj[i]=nn.Linear(in_feats,self.d_model).to(device=dev,dtype=dtype); self._inited[i]=True\n",
        "    def forward(self, x):\n",
        "        B,T,F=x.shape; outs=[]\n",
        "        for i,s in enumerate(self.scales):\n",
        "            size=max(4, self.seq_len//s)\n",
        "            step=max(1, int(round(size*(1.0-self.overlap))))\n",
        "            if T<size:\n",
        "                outs.append(torch.zeros(B,0,self.d_model,device=x.device,dtype=x.dtype)); continue\n",
        "            patches=x.unfold(dimension=1,size=size,step=step)\n",
        "            pooled=patches.mean(dim=2)\n",
        "            self._ensure(i, pooled.size(-1), pooled.device, pooled.dtype)\n",
        "            outs.append(self.proj[i](pooled))\n",
        "        return torch.cat(outs,dim=1)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads=8):\n",
        "        super().__init__(); self.mha=nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
        "    def forward(self, q, kv): out,_=self.mha(q,kv,kv); return out\n",
        "\n",
        "class MultiPatchFormerX_Reg(nn.Module):\n",
        "    def __init__(self, long_win, short_win, in_feats, d_model=160, nheads=8, nlayers=3,\n",
        "                 dropout=0.2, patch_overlap=0.75, scales=(1,2,4), out_dim=2):\n",
        "        super().__init__()\n",
        "        self.stem_long=ConvStem(in_feats,k=7,dropout=dropout*0.5)\n",
        "        self.stem_short=ConvStem(in_feats,k=5,dropout=dropout*0.5)\n",
        "        self.patch_long=TimePooledPatch(long_win,d_model,scales,patch_overlap)\n",
        "        self.patch_short=TimePooledPatch(short_win,d_model,scales,patch_overlap)\n",
        "        self.norm_long=nn.LayerNorm(d_model); self.norm_short=nn.LayerNorm(d_model)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "        self.cross_l2s=CrossAttention(d_model,nheads)\n",
        "        self.cross_s2l=CrossAttention(d_model,nheads)\n",
        "        enc_layer=nn.TransformerEncoderLayer(d_model,nheads,d_model*4,batch_first=True,dropout=dropout,activation='gelu')\n",
        "        self.encoder=nn.TransformerEncoder(enc_layer,num_layers=nlayers)\n",
        "        self.head=nn.Sequential(nn.Linear(d_model,128), nn.GELU(), nn.Dropout(dropout), nn.Linear(128,out_dim))\n",
        "    def forward(self, xs, xl):\n",
        "        xs=self.stem_short(xs); xl=self.stem_long(xl)\n",
        "        ps=self.patch_short(xs); pl=self.patch_long(xl)\n",
        "        ps=self.norm_short(ps+positional_encoding(ps.size(1),ps.size(2),ps.device)); ps=self.drop(ps)\n",
        "        pl=self.norm_long(pl+positional_encoding(pl.size(1),pl.size(2),pl.device));  pl=self.drop(pl)\n",
        "        ps2=self.cross_l2s(ps,pl)+ps; pl2=self.cross_s2l(pl,ps)+pl\n",
        "        z=torch.cat([ps2,pl2],dim=1); z=self.encoder(z)\n",
        "        rep=z.mean(dim=1)\n",
        "        return self.head(rep)\n",
        "\n",
        "def weighted_huber_2axis(pred, y, thresh_vec, wx=1.0, wy=1.0):\n",
        "    base = nn.functional.smooth_l1_loss(pred, y, reduction='none')\n",
        "    spike = torch.where(y.abs() >= thresh_vec, SPIKE_WEIGHT, 1.0)\n",
        "    w_axis = torch.tensor([wx, wy], device=pred.device, dtype=pred.dtype)\n",
        "    w = spike * w_axis\n",
        "    w = w / w.mean()\n",
        "    return (base * w).mean()\n",
        "\n",
        "def time_mask(x, L=6):\n",
        "    B,T,F = x.shape\n",
        "    if T <= L: return x\n",
        "    s = torch.randint(0, T-L+1, (B,), device=x.device)\n",
        "    for i in range(B):\n",
        "        x[i, s[i]:s[i]+L, :] = 0.0\n",
        "    return x\n",
        "\n",
        "class DualWinStreamDS(Dataset):\n",
        "\n",
        "    def __init__(self, df, input_cols, y_cols_norm, trial_col, mask,\n",
        "                 long_win, short_win, stride, horizon):\n",
        "        self.df = df; self.input_cols = input_cols; self.y_cols_norm = y_cols_norm\n",
        "        self.trial_col = trial_col\n",
        "        self.long_win = int(long_win); self.short_win = int(short_win)\n",
        "        self.stride = int(stride); self.horizon = int(horizon)\n",
        "        self.index = []; self.trial_groups = {}\n",
        "        for t, g in df.groupby(trial_col, sort=False):\n",
        "            g2 = g[mask.loc[g.index]]\n",
        "            if len(g2) < self.long_win + self.horizon: continue\n",
        "            self.trial_groups[str(t)] = g2\n",
        "            T = len(g2)\n",
        "            n = 1 + (T - self.long_win) // max(1, self.stride)\n",
        "            for i in range(n):\n",
        "                s = i * self.stride\n",
        "                tgt = s + self.long_win - 1 + self.horizon\n",
        "                if tgt < T:\n",
        "                    self.index.append((str(t), s))\n",
        "    def __len__(self): return len(self.index)\n",
        "    def __getitem__(self, i):\n",
        "        tid, s = self.index[i]; g = self.trial_groups[tid]\n",
        "        sub = g.iloc[s : s + self.long_win]\n",
        "        Xl = sub[self.input_cols].to_numpy(dtype=np.float32)\n",
        "        Xs = Xl[-self.short_win:, :]\n",
        "        tgt_pos = s + self.long_win - 1 + self.horizon\n",
        "        y_norm = g.iloc[tgt_pos][self.y_cols_norm].to_numpy(dtype=np.float32)\n",
        "        last_true_norm = sub[self.y_cols_norm].iloc[-1].to_numpy(dtype=np.float32)\n",
        "        return (torch.from_numpy(Xl), torch.from_numpy(Xs),\n",
        "                torch.from_numpy(y_norm), torch.from_numpy(last_true_norm),\n",
        "                tid, int(tgt_pos))\n",
        "\n",
        "def predict_tta(model, xs, xl, n=5, noise=0.01):\n",
        "    preds=[]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n):\n",
        "            xs_n = xs + (torch.randn_like(xs)*noise if noise>0 else 0.0)\n",
        "            xl_n = xl + (torch.randn_like(xl)*noise if noise>0 else 0.0)\n",
        "            preds.append(model(xs_n, xl_n))\n",
        "    return torch.stack(preds,0).mean(0)\n",
        "\n",
        "def measure_latency(model, loader, device, mode=\"plain\",\n",
        "                    warmup_steps=10, measure_steps=50,\n",
        "                    tta_runs=5, tta_noise=0.01):\n",
        "    import time as _time\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        w = 0\n",
        "        for xl_b, xs_b, y_b, last_b, _t, _ti in loader:\n",
        "            xl_b=xl_b.to(device); xs_b=xs_b.to(device)\n",
        "            _ = model(xs_b, xl_b) if mode==\"plain\" else predict_tta(model, xs_b, xl_b, n=tta_runs, noise=tta_noise)\n",
        "            w += 1\n",
        "            if w >= warmup_steps: break\n",
        "    total_time = 0.0; total_items = 0; steps = 0\n",
        "    with torch.no_grad():\n",
        "        for xl_b, xs_b, y_b, last_b, _t, _ti in loader:\n",
        "            xl_b=xl_b.to(device); xs_b=xs_b.to(device)\n",
        "            if device.type == 'cuda': torch.cuda.synchronize()\n",
        "            t0 = _time.perf_counter()\n",
        "            _ = model(xs_b, xl_b) if mode==\"plain\" else predict_tta(model, xs_b, xl_b, n=tta_runs, noise=tta_noise)\n",
        "            if device.type == 'cuda': torch.cuda.synchronize()\n",
        "            t1 = _time.perf_counter()\n",
        "            total_time  += (t1 - t0); total_items += xs_b.size(0); steps += 1\n",
        "            if steps >= measure_steps: break\n",
        "    if total_items == 0 or total_time == 0: return None, None\n",
        "    ms_per_item = (total_time / total_items) * 1000.0; items_per_s = total_items / total_time\n",
        "    return ms_per_item, items_per_s\n",
        "\n",
        "def measure_latency_per_sample(model, loader, device, mode=\"plain\",\n",
        "                               warmup_samples=20, measure_samples=50,\n",
        "                               tta_runs=5, tta_noise=0.01):\n",
        "    import time as _time\n",
        "    model.eval(); done = 0\n",
        "    with torch.no_grad():\n",
        "        for xl_b, xs_b, y_b, last_b, _t, _ti in loader:\n",
        "            for i in range(xl_b.size(0)):\n",
        "                xs = xs_b[i:i+1].to(device); xl = xl_b[i:i+1].to(device)\n",
        "                _ = model(xs, xl) if mode==\"plain\" else predict_tta(model, xs, xl, n=tta_runs, noise=tta_noise)\n",
        "                done += 1\n",
        "                if done >= warmup_samples: break\n",
        "            if done >= warmup_samples: break\n",
        "    total_time = 0.0; total_items = 0; done = 0\n",
        "    with torch.no_grad():\n",
        "        for xl_b, xs_b, y_b, last_b, _t, _ti in loader:\n",
        "            for i in range(xl_b.size(0)):\n",
        "                xs = xs_b[i:i+1].to(device); xl = xl_b[i:i+1].to(device)\n",
        "                if device.type == 'cuda': torch.cuda.synchronize()\n",
        "                t0 = _time.perf_counter()\n",
        "                _ = model(xs, xl) if mode==\"plain\" else predict_tta(model, xs, xl, n=tta_runs, noise=tta_noise)\n",
        "                if device.type == 'cuda': torch.cuda.synchronize()\n",
        "                t1 = _time.perf_counter()\n",
        "                total_time += (t1 - t0); total_items += 1; done += 1\n",
        "                if done >= measure_samples: break\n",
        "            if done >= measure_samples: break\n",
        "    if total_items == 0 or total_time == 0.0: return None, None\n",
        "    ms_per_sample = (total_time / total_items) * 1000.0; samples_per_s = total_items / total_time\n",
        "    return ms_per_sample, samples_per_s\n",
        "\n",
        "\n",
        "set_seed()\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "df[num_cols] = df[num_cols].astype(np.float32, copy=False)\n",
        "\n",
        "if TARGET_X not in df.columns or TARGET_Y not in df.columns:\n",
        "    raise ValueError(\"Missing newCopX / newCopY in CSV.\")\n",
        "tcol = find_time_col(df)\n",
        "print(f\"[LOAD] rows={len(df)} | trials={df[TRIAL_COL].nunique()}\")\n",
        "\n",
        "if USE_DERIVED_FEATS:\n",
        "    df = add_derived_feats(df)\n",
        "\n",
        "base_inputs = find_inputs(df)\n",
        "\n",
        "numeric_cols=[c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "feat_cols=[c for c in base_inputs if c in numeric_cols]\n",
        "tgt_cols=[c for c in [TARGET_X,TARGET_Y] if c in numeric_cols]\n",
        "\n",
        "\n",
        "if CLEAN_LEVEL>0:\n",
        "    robust_clip_inplace(df, feat_cols+tgt_cols,\n",
        "                        by=(PARTIC_COL if (PARTIC_COL and PARTIC_COL in df.columns) else None),\n",
        "                        k=MAD_K)\n",
        "    if HAMPEL_K>0:\n",
        "        try:\n",
        "            hampel_targets_per_trial_inplace(df, tgt_cols, k=HAMPEL_K, t0=HAMPEL_T0)\n",
        "        except MemoryError:\n",
        "            print(\"[WARN] Hampel OOM -> skipping Hampel.\")\n",
        "\n",
        "arr = df[feat_cols+tgt_cols].to_numpy(copy=False)\n",
        "bad = ~np.isfinite(arr)\n",
        "if bad.any():\n",
        "    arr = arr.copy(); arr[bad] = np.nan\n",
        "    df[feat_cols+tgt_cols] = arr\n",
        "df = df.dropna(subset=feat_cols+tgt_cols).reset_index(drop=True)\n",
        "print(f\"[CLEAN] rows={len(df)} | trials={df[TRIAL_COL].nunique()} after cleaning\")\n",
        "print(f\"[INFO] Using {len(feat_cols)} input features.\")\n",
        "\n",
        "fs = estimate_fs(df, tcol, default=50.0)\n",
        "long_len  = seconds_to_samples(DES_LONG_SEC,  fs)\n",
        "short_len = seconds_to_samples(DES_SHORT_SEC, fs)\n",
        "stride    = max(1, seconds_to_samples(DES_HOP_SEC, fs))\n",
        "horizon   = seconds_to_samples(DES_HOR_SEC, fs)\n",
        "\n",
        "unique_parts = sorted(df[PARTIC_COL].unique())\n",
        "n_splits_cv = 10\n",
        "print(f\"[CV] Using {n_splits_cv}-fold CV, stratified by participant ({len(unique_parts)} participants).\")\n",
        "\n",
        "\n",
        "fold_metrics = []\n",
        "\n",
        "for fold_idx in range(1, n_splits_cv + 1):\n",
        "    np.random.seed(RANDOM_SEED + fold_idx)\n",
        "\n",
        "    train_trials = []; val_trials = []; test_trials = []\n",
        "    for p in unique_parts:\n",
        "        their_trials = df[df[PARTIC_COL] == p][TRIAL_COL].unique()\n",
        "        if len(their_trials) < 7:\n",
        "            raise ValueError(f\"Participant {p} has <7 trials; adjust splitting.\")\n",
        "        np.random.shuffle(their_trials)\n",
        "        train_trials.extend(their_trials[:5])\n",
        "        val_trials.append(their_trials[5])\n",
        "        test_trials.append(their_trials[6])\n",
        "\n",
        "    print(f\"\\n FOLD {fold_idx}/{n_splits_cv}\")\n",
        "    print(f\"Trials: train={len(train_trials)}, val={len(val_trials)}, test={len(test_trials)}\")\n",
        "\n",
        "    train_mask = df[TRIAL_COL].isin(train_trials)\n",
        "    val_mask   = df[TRIAL_COL].isin(val_trials)\n",
        "    test_mask  = df[TRIAL_COL].isin(test_trials)\n",
        "\n",
        "    df_fold = df.copy(deep=True)\n",
        "\n",
        "\n",
        "    train_targets = df_fold.loc[train_mask, [TARGET_X, TARGET_Y]]\n",
        "    gmx = float(np.median(train_targets[TARGET_X])); gmy = float(np.median(train_targets[TARGET_Y]))\n",
        "    gsx = float(np.subtract(*np.percentile(train_targets[TARGET_X], [75,25]))); gsx = max(gsx, 1e-6)\n",
        "    gsy = float(np.subtract(*np.percentile(train_targets[TARGET_Y], [75,25]))); gsy = max(gsy, 1e-6)\n",
        "\n",
        "    df_fold['tx_norm'] = ((df_fold[TARGET_X] - gmx) / gsx).astype(np.float32)\n",
        "    df_fold['ty_norm'] = ((df_fold[TARGET_Y] - gmy) / gsy).astype(np.float32)\n",
        "    ycols_norm = ['tx_norm','ty_norm']\n",
        "\n",
        "\n",
        "    fold_input_cols = list(feat_cols)\n",
        "    x_scaler = RobustScaler().fit(df_fold.loc[train_mask, fold_input_cols])\n",
        "    df_fold[fold_input_cols] = x_scaler.transform(df_fold[fold_input_cols])\n",
        "\n",
        "\n",
        "    train_ds = DualWinStreamDS(df_fold, fold_input_cols, ycols_norm, TRIAL_COL, train_mask,\n",
        "                               long_len, short_len, stride, horizon)\n",
        "    val_ds   = DualWinStreamDS(df_fold, fold_input_cols, ycols_norm, TRIAL_COL, val_mask,\n",
        "                               long_len, short_len, stride, horizon) if val_mask.any() else None\n",
        "    test_ds  = DualWinStreamDS(df_fold, fold_input_cols, ycols_norm, TRIAL_COL, test_mask,\n",
        "                               long_len, short_len, stride, horizon)\n",
        "    print(f\"[WIN] train={len(train_ds)}, val={0 if val_ds is None else len(val_ds)}, test={len(test_ds)}\")\n",
        "\n",
        "    pin = (DEVICE.type=='cuda')\n",
        "    train_loader=DataLoader(train_ds, batch_size=BATCH, shuffle=True,  pin_memory=pin)\n",
        "    val_loader  =DataLoader(val_ds,   batch_size=BATCH, shuffle=False, pin_memory=pin) if (val_ds and len(val_ds)) else None\n",
        "    test_loader =DataLoader(test_ds,  batch_size=BATCH, shuffle=False, pin_memory=pin)\n",
        "\n",
        "    model=MultiPatchFormerX_Reg(long_win=long_len, short_win=short_len, in_feats=len(fold_input_cols),\n",
        "                                d_model=D_MODEL, nheads=N_HEADS, nlayers=N_LAYERS,\n",
        "                                dropout=DROPOUT, patch_overlap=PATCH_OVERLAP, scales=SCALES, out_dim=2).to(DEVICE)\n",
        "    opt=torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    if USE_SWA:\n",
        "        from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "        swa_model = nn.Identity(); swa_start_epoch = int(max(1, EPOCHS * SWA_START_FRAC))\n",
        "        swa_scheduler = None\n",
        "\n",
        "\n",
        "    if len(train_ds):\n",
        "        train_targets_n = df_fold.loc[train_mask, ycols_norm].to_numpy(dtype=np.float32)\n",
        "        thr_vals = np.quantile(np.abs(train_targets_n), WEIGHT_Q, axis=0)\n",
        "        spike_thr = torch.tensor(thr_vals, dtype=torch.float32, device=DEVICE)\n",
        "    else:\n",
        "        spike_thr = torch.tensor([1.0,1.0], dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "    wx = 1.0; wy = 1.0\n",
        "    if AXIS_WEIGHT_FROM_TRAIN_IQR: wy = gsy / (gsx + 1e-6)\n",
        "\n",
        "\n",
        "    if len(train_ds):\n",
        "        with torch.no_grad():\n",
        "            xl_b, xs_b, y_b, last_b, _tid, _tidx = next(iter(train_loader))\n",
        "            _=model(xs_b.to(DEVICE), xl_b.to(DEVICE))\n",
        "        print(\"[DEBUG] Forward sanity check OK.\")\n",
        "\n",
        "\n",
        "    best_val=np.inf; best_state=None; bad=0\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        if not len(train_ds): break\n",
        "        model.train(); run=0.0\n",
        "        for xl_b, xs_b, y_b, last_b, _tid, _tidx in train_loader:\n",
        "            xl_b=xl_b.to(DEVICE); xs_b=xs_b.to(DEVICE); y_b=y_b.to(DEVICE)\n",
        "            if USE_AUG:\n",
        "                if AUG_NOISE_STD>0:\n",
        "                    xl_b = xl_b + torch.randn_like(xl_b)*AUG_NOISE_STD\n",
        "                    xs_b = xs_b + torch.randn_like(xs_b)*AUG_NOISE_STD\n",
        "                if np.random.rand() < AUG_TIMEMASK_P:\n",
        "                    xl_b = time_mask(xl_b, L=AUG_TIMEMASK_LEN)\n",
        "                    xs_b = time_mask(xs_b, L=AUG_TIMEMASK_LEN)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            pred=model(xs_b, xl_b)\n",
        "            loss=weighted_huber_2axis(pred, y_b, spike_thr, wx=wx, wy=wy)\n",
        "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "            opt.step(); run+=float(loss.item())\n",
        "\n",
        "        if USE_SWA and epoch == swa_start_epoch:\n",
        "            swa_model = AveragedModel(model)\n",
        "            swa_scheduler = SWALR(opt, anneal_strategy='cos', anneal_epochs=5, swa_lr=LR * SWA_LR_FACTOR)\n",
        "\n",
        "        if val_loader and len(val_ds):\n",
        "            model.eval(); ya=[]; yh=[]\n",
        "            with torch.no_grad():\n",
        "                for xl_b, xs_b, y_b, last_b, t_b, t_idx in val_loader:\n",
        "                    xl_b=xl_b.to(DEVICE); xs_b=xs_b.to(DEVICE); y_b=y_b.to(DEVICE)\n",
        "                    pred=model(xs_b, xl_b); ya.append(y_b.cpu().numpy()); yh.append(pred.cpu().numpy())\n",
        "            y_true_n = np.concatenate(ya,0); y_pred_n = np.concatenate(yh,0)\n",
        "            y_true = np.stack([y_true_n[:,0]*gsx+gmx, y_true_n[:,1]*gsy+gmy], axis=1)\n",
        "            y_pred = np.stack([y_pred_n[:,0]*gsx+gmx, y_pred_n[:,1]*gsy+gmy], axis=1)\n",
        "            v_mae  = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "            if USE_SWA and epoch >= swa_start_epoch:\n",
        "                swa_model.update_parameters(model)\n",
        "                if swa_scheduler is not None: swa_scheduler.step()\n",
        "            else:\n",
        "                scheduler.step(v_mae)\n",
        "\n",
        "            print(f\"[F{fold_idx} E{epoch:02d}] train_loss={run/max(1,len(train_loader)):.4f} | val MAE={v_mae:.3f}\")\n",
        "            if v_mae<best_val:\n",
        "                best_val=v_mae; best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}; bad=0\n",
        "            else:\n",
        "                bad+=1\n",
        "                if bad>=PATIENCE: print(\"[EARLY STOP]\"); break\n",
        "        else:\n",
        "            print(f\"[F{fold_idx} E{epoch:02d}] train_loss={run/max(1,len(train_loader)):.4f}\")\n",
        "            if USE_SWA and epoch >= swa_start_epoch:\n",
        "                swa_model.update_parameters(model)\n",
        "                if swa_scheduler is not None: swa_scheduler.step()\n",
        "\n",
        "    if best_state is not None and len(train_ds): model.load_state_dict(best_state)\n",
        "    if USE_SWA and isinstance(swa_model, nn.Module) and not isinstance(swa_model, nn.Identity):\n",
        "        model = swa_model\n",
        "\n",
        "\n",
        "    lat_plain_batch = lat_tta_batch = lat_plain_ps = lat_tta_ps = (np.nan, np.nan)\n",
        "    if PRINT_LATENCY:\n",
        "        lat_plain_batch = measure_latency(model, test_loader, DEVICE, mode=\"plain\",\n",
        "                                          warmup_steps=LAT_WARMUP_STEPS, measure_steps=LAT_MEASURE_STEPS,\n",
        "                                          tta_runs=TTA_RUNS, tta_noise=TTA_NOISE)\n",
        "        print(f\"[LATENCY] Plain forward (batches): {lat_plain_batch[0]:.2f} ms/sample | {lat_plain_batch[1]:.1f} samples/s\")\n",
        "        lat_plain_ps = measure_latency_per_sample(model, test_loader, DEVICE, mode=\"plain\",\n",
        "                                                  warmup_samples=LAT_WARMUP_SAMPLES_PS, measure_samples=LAT_MEASURE_SAMPLES_PS,\n",
        "                                                  tta_runs=TTA_RUNS, tta_noise=TTA_NOISE)\n",
        "        print(f\"[LATENCY] Plain per-sample: {lat_plain_ps[0]:.2f} ms/sample | {lat_plain_ps[1]:.1f} samples/s\")\n",
        "        if USE_TTA:\n",
        "            lat_tta_batch = measure_latency(model, test_loader, DEVICE, mode=\"tta\",\n",
        "                                            warmup_steps=max(5, LAT_WARMUP_STEPS//2),\n",
        "                                            measure_steps=max(10, LAT_MEASURE_STEPS//2),\n",
        "                                            tta_runs=TTA_RUNS, tta_noise=TTA_NOISE)\n",
        "            print(f\"[LATENCY] TTA (batches): {lat_tta_batch[0]:.2f} ms/sample | {lat_tta_batch[1]:.1f} samples/s\")\n",
        "            lat_tta_ps = measure_latency_per_sample(model, test_loader, DEVICE, mode=\"tta\",\n",
        "                                                    warmup_samples=max(10, LAT_WARMUP_SAMPLES_PS//2),\n",
        "                                                    measure_samples=max(30, LAT_MEASURE_SAMPLES_PS//2),\n",
        "                                                    tta_runs=TTA_RUNS, tta_noise=TTA_NOISE)\n",
        "            print(f\"[LATENCY] TTA per-sample: {lat_tta_ps[0]:.2f} ms/sample | {lat_tta_ps[1]:.1f} samples/s\")\n",
        "\n",
        "\n",
        "    model.eval(); ya=[]; yh=[]; ylast=[]; tids=[]; tidxs=[]\n",
        "    with torch.no_grad():\n",
        "        for xl_b, xs_b, y_b, last_b, t_b, t_idx in test_loader:\n",
        "            xl_b=xl_b.to(DEVICE); xs_b=xs_b.to(DEVICE)\n",
        "            pred = predict_tta(model, xs_b, xl_b, n=TTA_RUNS, noise=TTA_NOISE) if USE_TTA else model(xs_b, xl_b)\n",
        "            ya.append(y_b.cpu().numpy()); yh.append(pred.cpu().numpy()); ylast.append(last_b.cpu().numpy())\n",
        "            tids += list(t_b); tidxs += list(t_idx.numpy())\n",
        "\n",
        "    y_true_n = np.concatenate(ya,0) if ya else np.zeros((0,2),np.float32)\n",
        "    y_pred_n = np.concatenate(yh,0) if yh else np.zeros((0,2),np.float32)\n",
        "    y_last_n = np.concatenate(ylast,0) if ylast else np.zeros((0,2),np.float32)\n",
        "\n",
        "\n",
        "    y_true = np.stack([y_true_n[:,0]*gsx+gmx, y_true_n[:,1]*gsy+gmy], axis=1)\n",
        "    y_pred = np.stack([y_pred_n[:,0]*gsx+gmx, y_pred_n[:,1]*gsy+gmy], axis=1)\n",
        "    y_pers = np.stack([y_last_n[:,0]*gsx+gmx, y_last_n[:,1]*gsy+gmy], axis=1)\n",
        "\n",
        "    # Metrics\n",
        "    if len(y_true):\n",
        "        mae_x=mean_absolute_error(y_true[:,0], y_pred[:,0]); mae_y=mean_absolute_error(y_true[:,1], y_pred[:,1])\n",
        "        mse_x=mean_squared_error(y_true[:,0], y_pred[:,0]);  mse_y=mean_squared_error(y_true[:,1], y_pred[:,1])\n",
        "        rmse_x=np.sqrt(mse_x); rmse_y=np.sqrt(mse_y)\n",
        "        r2_x=r2_score(y_true[:,0], y_pred[:,0]); r2_y=r2_score(y_true[:,1], y_pred[:,1])\n",
        "\n",
        "        mae_x_p=mean_absolute_error(y_true[:,0], y_pers[:,0]); mae_y_p=mean_absolute_error(y_true[:,1], y_pers[:,1])\n",
        "        mse_x_p=mean_squared_error(y_true[:,0], y_pers[:,0]);  mse_y_p=mean_squared_error(y_true[:,1], y_pers[:,1])\n",
        "        rmse_x_p=np.sqrt(mse_x_p); rmse_y_p=np.sqrt(mse_y_p)\n",
        "        r2_x_p=r2_score(y_true[:,0], y_pers[:,0]); r2_y_p=r2_score(y_true[:,1], y_pers[:,1])\n",
        "    else:\n",
        "        mae_x=mae_y=mse_x=mse_y=rmse_x=rmse_y=r2_x=r2_y=np.nan\n",
        "        mae_x_p=mae_y_p=mse_x_p=mse_y_p=rmse_x_p=rmse_y_p=r2_x_p=r2_y_p=np.nan\n",
        "\n",
        "    print(f\"[FOLD {fold_idx}] \"\n",
        "          f\"MAE X={mae_x:.3f} Y={mae_y:.3f} | \"\n",
        "          f\"MSE X={mse_x:.3f} Y={mse_y:.3f} | \"\n",
        "          f\"RMSE X={rmse_x:.3f} Y={rmse_y:.3f} | \"\n",
        "          f\"R^2 X={r2_x:.3f} Y={r2_y:.3f} || \"\n",
        "          f\"[PERSIST] MAE X={mae_x_p:.3f} Y={mae_y_p:.3f} | \"\n",
        "          f\"MSE X={mse_x_p:.3f} Y={mse_y_p:.3f} | \"\n",
        "          f\"RMSE X={rmse_x_p:.3f} Y={rmse_y_p:.3f} | \"\n",
        "          f\"R^2 X={r2_x_p:.3f} Y={r2_y_p:.3f}\")\n",
        "\n",
        "    fold_metrics.append({\n",
        "        \"mae_x\": mae_x, \"mae_y\": mae_y,\n",
        "        \"mse_x\": mse_x, \"mse_y\": mse_y,\n",
        "        \"rmse_x\": rmse_x, \"rmse_y\": rmse_y,\n",
        "        \"r2_x\": r2_x, \"r2_y\": r2_y,\n",
        "        \"mae_x_p\": mae_x_p, \"mae_y_p\": mae_y_p,\n",
        "        \"mse_x_p\": mse_x_p, \"mse_y_p\": mse_y_p,\n",
        "        \"rmse_x_p\": rmse_x_p, \"rmse_y_p\": rmse_y_p,\n",
        "        \"r2_x_p\": r2_x_p, \"r2_y_p\": r2_y_p,\n",
        "        \"lat_plain_ps_ms\": lat_plain_ps[0], \"lat_plain_ps_sps\": lat_plain_ps[1],\n",
        "        \"lat_tta_ps_ms\": lat_tta_ps[0] if USE_TTA else np.nan,\n",
        "        \"lat_tta_ps_sps\": lat_tta_ps[1] if USE_TTA else np.nan\n",
        "    })\n",
        "\n",
        "\n",
        "    if len(y_true):\n",
        "\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(11, 4))\n",
        "        labels = ['X', 'Y']\n",
        "        for i in range(2):\n",
        "            ax[i].scatter(y_true[:, i], y_pred[:, i], s=6, alpha=0.5)\n",
        "            lo = float(min(y_true[:, i].min(), y_pred[:, i].min()))\n",
        "            hi = float(max(y_true[:, i].max(), y_pred[:, i].max()))\n",
        "            ax[i].plot([lo, hi], [lo, hi], 'k--', lw=1)\n",
        "            ax[i].set_xlabel('True'); ax[i].set_ylabel('Pred')\n",
        "            ax[i].set_title(f'Fold {fold_idx}: newCoP {labels[i]} — True vs Pred')\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "        series = {}\n",
        "        for i, (tid, ti) in enumerate(zip(tids, tidxs)):\n",
        "            if tid not in series: series[tid] = {'t': [], 'tx': [], 'px': [], 'ty': [], 'py': []}\n",
        "            series[tid]['t'].append(ti)\n",
        "            series[tid]['tx'].append(y_true[i,0]); series[tid]['px'].append(y_pred[i,0])\n",
        "            series[tid]['ty'].append(y_true[i,1]); series[tid]['py'].append(y_pred[i,1])\n",
        "\n",
        "        plot_trials = [str(t) for t in test_trials if str(t) in series]\n",
        "        for tid in plot_trials:\n",
        "            s = series[tid]\n",
        "            order = np.argsort(s['t'])\n",
        "            t_sec = (np.asarray(s['t'])[order] / fs).astype(float)\n",
        "            tx = np.asarray(s['tx'])[order]; px = np.asarray(s['px'])[order]\n",
        "            ty = np.asarray(s['ty'])[order]; py = np.asarray(s['py'])[order]\n",
        "\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.plot(t_sec, tx, label='True newCopX')\n",
        "            plt.plot(t_sec, px, label='Pred newCopX')\n",
        "            plt.title(f'Fold {fold_idx} | Trial {tid}: Time Series newCopX')\n",
        "            plt.xlabel('Time (s)'); plt.ylabel('newCopX'); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.plot(t_sec, ty, label='True newCopY')\n",
        "            plt.plot(t_sec, py, label='Pred newCopY')\n",
        "            plt.title(f'Fold {fold_idx} | Trial {tid}: Time Series newCopY')\n",
        "            plt.xlabel('Time (s)'); plt.ylabel('newCopY'); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "    save_model = MultiPatchFormerX_Reg(\n",
        "        long_win=long_len, short_win=short_len, in_feats=len(fold_input_cols),\n",
        "        d_model=D_MODEL, nheads=N_HEADS, nlayers=N_LAYERS,\n",
        "        dropout=DROPOUT, patch_overlap=PATCH_OVERLAP, scales=SCALES, out_dim=2\n",
        "    )\n",
        "    raw_state = model.state_dict()\n",
        "    clean_state = {k.replace(\"module.\", \"\"): v.cpu() for k, v in raw_state.items()}\n",
        "    save_model.load_state_dict(clean_state, strict=False)\n",
        "    save_model.cpu().eval()\n",
        "    ckpt = {\n",
        "        \"model_state\": save_model.state_dict(),\n",
        "        \"model_cfg\": {\n",
        "            \"long_win\": long_len, \"short_win\": short_len, \"in_feats\": len(fold_input_cols),\n",
        "            \"d_model\": D_MODEL, \"nheads\": N_HEADS, \"nlayers\": N_LAYERS, \"dropout\": DROPOUT,\n",
        "            \"patch_overlap\": PATCH_OVERLAP, \"scales\": SCALES, \"out_dim\": 2\n",
        "        },\n",
        "        \"input_cols\": fold_input_cols,\n",
        "        \"x_scaler\": x_scaler,\n",
        "        \"global_target_stats\": {\"gmx\": gmx, \"gmy\": gmy, \"gsx\": gsx, \"gsy\": gsy},\n",
        "        \"fs\": fs, \"horizon\": horizon, \"stride\": stride,\n",
        "        \"ycols_norm\": ycols_norm, \"seed\": RANDOM_SEED, \"fold_idx\": fold_idx\n",
        "    }\n",
        "    ckpt_path = CKPT_DIR / f\"mpf_xreg_fair_fold{fold_idx}.pt\"\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "    print(f\"[SAVE] Checkpoint saved to: {ckpt_path}\")\n",
        "\n",
        "\n",
        "def mean_std(arr):\n",
        "    arr = np.asarray(arr, float)\n",
        "    mu = float(np.nanmean(arr))\n",
        "    finite = np.isfinite(arr)\n",
        "    sd = float(np.nanstd(arr[finite], ddof=1)) if np.sum(finite) > 1 else float(np.nan)\n",
        "    return mu, sd\n",
        "\n",
        "mae_x_vals  = [m[\"mae_x\"]  for m in fold_metrics]\n",
        "mae_y_vals  = [m[\"mae_y\"]  for m in fold_metrics]\n",
        "mse_x_vals  = [m[\"mse_x\"]  for m in fold_metrics]\n",
        "mse_y_vals  = [m[\"mse_y\"]  for m in fold_metrics]\n",
        "rmse_x_vals = [m[\"rmse_x\"] for m in fold_metrics]\n",
        "rmse_y_vals = [m[\"rmse_y\"] for m in fold_metrics]\n",
        "r2_x_vals   = [m[\"r2_x\"]   for m in fold_metrics]\n",
        "r2_y_vals   = [m[\"r2_y\"]   for m in fold_metrics]\n",
        "\n",
        "def report(name, vals):\n",
        "    mu, sd = mean_std(vals); print(f\"{name:<22}: {mu:.3f} ± {sd:.3f}\")\n",
        "\n",
        "print(\"\\n=== Cross-Fold Summary (Model) ===\")\n",
        "report(\"MAE X\", mae_x_vals);   report(\"MAE Y\", mae_y_vals)\n",
        "report(\"MSE X\", mse_x_vals);   report(\"MSE Y\", mse_y_vals)\n",
        "report(\"RMSE X\", rmse_x_vals); report(\"RMSE Y\", rmse_y_vals)\n",
        "report(\"R^2 X\", r2_x_vals);    report(\"R^2 Y\", r2_y_vals)\n",
        "\n",
        "\n",
        "for i, m in enumerate(fold_metrics, 1):\n",
        "    print(f\"[FOLD {i}] Latency per-sample (plain): {m['lat_plain_ps_ms']:.2f} ms/sample | {m['lat_plain_ps_sps']:.1f} samples/s\")\n",
        "    if not np.isnan(m['lat_tta_ps_ms']):\n",
        "        print(f\"[FOLD {i}] Latency per-sample (TTA):   {m['lat_tta_ps_ms']:.2f} ms/sample | {m['lat_tta_ps_sps']:.1f} samples/s\")\n"
      ]
    }
  ]
}